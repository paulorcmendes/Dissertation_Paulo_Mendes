%% -*- coding: utf-8 -*-
\newpage

\chapter{Conclusions}
\label{chap:conclusions}

In this dissertation, we present a method for spatiotemporal localization of actors based on \emph{Video Face Clustering}. As part of this method, we also define an algorithm for finding an adequate number of clusters based on the silhouette score. We investigate in what extent this method can be used as the core to leverage and enhance some innovative applications, specially in three different  practical and important tasks: Video Face Recognition, Educational Video Recommendation, and Subtitles Positioning in 360-Video.

For the Video Face Recognition, we propose a cluster-matching-based approach, derived mainly by the characteristics of our core Face Clustering method, which is very scalable since the the effort spent with annotations is significantly reduced --- as it is done over clusters instead of single images. This method uses \emph{video face clustering} and a heuristic for cluster matching in order to recognize people in video. It has achieved a recall of 99.435\% and a precision of 99.131\% when considering faces extracted from a set of 13 video files. As another consequence of face clustering, our technique can be useful for creating and labeling datasets on a less time-consuming way by labeling clusters instead of individual images. One of the limitations of this application is related to the size of the video set used for the overall evaluation of our method. This is due to the difficulty of finding videos where it is possible to manually identify in each frame whether each person present is registered or not in our labeled clusters.

For the Educational Video Recommendation task, we investigate a new feature that can be used for video such recommendation: the presence of specific lecturers, which again is a direct result of the application of our core method. After performing \emph{video face clustering} on each video, we extract their centroids to perform another clustering step that creates a relationship of videos that share the presence of the same lecturers. Finally, we rank the recommended videos based on the amount of time that each lecturer is present. Our method uses only the video files for performing recommendation, no other information about these videos nor the identity of the lecturers is necessary. It is worth mentioning that we do not intend to substitute other video recommendation methods, rather our application shows that, if the presence of lecturers is a relevant feature for educational video recommendation, it can be used for this purpose with a mAP value of 0.99. The main limitation of this application is that we can only recommend videos in which the lecturers are visually present. As future work, we intend to investigate a hybrid recommendation approach, that combines both textual and audiovisual information from the video to create clusters.

For the Subtitles Positioning in 360-Video, our main contribution is the proposal of a dynamic placement of subtitles based on the automatic localization of actors and on the current visual position of the viewer. To achieve this goal, we adapted our spatiotemporal localization method to the 360 setting and created an authoring model for interactive 360-videos. Because of the severe distortions present in equirectangular 360-videos, we used an approach based on viewports extraction for the \emph{face detection} step of \emph{video face clustering}. In order to evaluate this approach against the sole use of a traditional CNN, we created a synthetic dataset by projecting images from the FDDB benchmark to equirectangular backgrounds. Our approach, that consists in using viewports with a traditional CNN, performed better than using the traditional CNN directly to the equirectangular images. Moreover, we proposed an authoring model that allows authors to design and create
interactive 360 videos. The proposed model is the result of the analysis of different scenarios of immersive 360 multimedia applications. Besides supporting subtitles positioning in 360-videos, it also supports navigation among 360-videos, additional media, etc. For other case studies not closely related to this dissertation, please check \cite{mendes2020authoring}. Currently, the model can be easily used through text editors with an implementation integrated with the Unity Game Engine\footnote{\url{https://github.com/TeleMidia/VR360Authoring}}.

In summary, we highlight the following contributions of this dissertation:
%%
\begin{enumerate}
    \item An algorithm that iteratively finds an adequate number clusters based on the silhouette score;
    \item A method for video face recognition that is easily scalable and helps in labelling images dataset;
    \item A method for educational video recommendation based on the presence of lecturers;
    \item A method for face detection in equirectangular 360-images that uses models pre-trained on common 2d images;
    \item A synthetic dataset for face detection in 360-images;
    \item An authoring model for interactive 360-videos;
    \item An approach for positioning subtitles in 360-videos based on the actors positions and the user's viewport;
\end{enumerate}

The choice for our three proposed applications was due to observations of opportunities to apply \emph{Video Face Clustering} in different contexts. We started with Video Face Recognition. We observed that the the use of only the \emph{Video Face Clustering} part of the method could generate the time segments that each actor is present without having to identify them. Then, we hypothesised that this particular feature could also be used to clustering actors in different videos. From this observation, we decided to investigate a method for \emph{Educational Video Recommendation} using this clustering of actors in different videos. The idea for \emph{Subtitles Positioning in 360-video} came from the authoring model we developed. We observed that we could automatically identify the actors using \emph{Video Face Clustering} so that the subtitles could follow the speakers in the 360-video. Thus, we adapted \emph{Video Face Clustering} for the 360-video setting. It is worth noticing that with this adaptation, the other two applications~(\emph{Video Face Recognition} and \emph{Educational Video Recommendation}) could also be applied to the context of 360-videos.

This work has opened lots of branches for future research. For the particular case of subtitles positioning in 360-video, there is still much to explore. It is not yet clear what is the most adequate way to place subtitles relatively to the actors positions. For instance: is it better to place them above or bellow the actors face? Should the subtitles follow the actors immediately or it would be better to have a delay with a smooth transition? A possible path towards answering these questions is by experimenting and evaluating these and other possible settings with users.

Other possible future work come from the combination of the applications we explored in this dissertation. For instance, the identification of lecturers using a lecturer's image dataset with \emph{Video Face Recognition} could improve the task of \emph{Educational Video Recommendation} by inserting additional relationships among lecturers~(e.g. lecturers that teach the same subject). Another possibility is to use \emph{Video Face Recognition} to identify and insert the name of the actor present in 360-videos together with the subtitles, so that the user's know who is talking when the actor is not visible to them.

Although this dissertation has focused on faces, the methods and techniques here investigated could also be applied in other contexts. For instance, we could recommend videos based on the presence of the same \emph{actions} in different videos, so that a reference video with many actions related to culinary such as pouring and chopping should have similar video as recommended. For doing that, we should be able to represent each action identified as an embedding~(vector). Then, the remainder of our method for \emph{Educational Video Recommendation} could be used. In the context of 360-videos, if we could generate embeddings from pieces of art, we could apply the process of \emph{Video Face Clustering}~(or \emph{Video Art Clustering} in this case) to detect and determine the video segments that each piece of art is present. By doing that, we would be able to use our authoring model to insert additional media such as text and audio to pieces of art in an interactive 360-video. This application could be very useful in the context of virtual tours in museums and historical sites.


\section{Publications}

 As results of this research, three papers have already been published at relevant multimedia conferences \cite{mendes2020cluster,mendes2020ISM, mendes2020authoring}. In \cite{mendes2020cluster}, we have evaluated video face clustering together with a cluster-matching method for video face recognition. In \cite{mendes2020ISM}, we have used video face clustering and the presence of actors in different video as a mean for recommending educational videos. In \cite{mendes2020authoring}, we have developed an authoring model and a player for interactive 360-video. 